{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "from env import Environment\n",
    "from main import Config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline: Fictitious Play Solving Two-Agents Dilemma\n",
    "Payoff matrix:\n",
    "\n",
    "| Tables      | Cooperation |  Defection |\n",
    "|-------------|:-----------:|-----------:|\n",
    "| Cooperation |   REWARD    | TEMPTATION |\n",
    "| Defection   |   SUCKER    | PUNISHMENT |"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "N_EPISODES = 100\n",
    "EPOCH = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [],
   "source": [
    "def two_agent_fictitious_play(payoff:Type.TensorType, reward:Type.TensorType, strategy:Type.TensorType, belief:Type.TensorType, epsilon:float=0.05, random:bool=False):\n",
    "    # exploration\n",
    "    for i in range(1,10):\n",
    "        strategy[i,0] = np.random.randint(0,2)\n",
    "        strategy[i,1] = np.random.randint(0,2)\n",
    "\n",
    "        # update the empirical distribution\n",
    "        belief[i,0] = ((i-1)*belief[i-1,0] + strategy[i,0])/i\n",
    "        belief[i,1] = ((i-1)*belief[i-1,1] + strategy[i,1])/i\n",
    "\n",
    "    for i in range(10,N_EPISODES):\n",
    "        # calculate payoff of Agent 0 assuming the other Agent plays according to empirical dist.\n",
    "        reward[0,0] = belief[i-1,1]*payoff[0,1] + (1-belief[i-1,1])*payoff[0,0]\n",
    "        reward[1,0] = belief[i-1,1]*payoff[1,1] + (1-belief[i-1,1])*payoff[1,0]\n",
    "\n",
    "        # calculate payoff of Agent 1 assuming the other Agent plays according to empirical dist.\n",
    "        reward[0,1] = belief[i-1,0]*payoff[0,1] + (1-belief[i-1,0])*payoff[0,0]\n",
    "        reward[1,1] = belief[i-1,0]*payoff[1,1] + (1-belief[i-1,0])*payoff[1,0]\n",
    "\n",
    "        # Agent 1's best response\n",
    "        if reward[0,0] < reward[1,0]-epsilon:\n",
    "            strategy[i,0] = 1\n",
    "        elif reward[0,0]-epsilon > reward[1,0]:\n",
    "            strategy[i,0] = 0\n",
    "        else:\n",
    "            strategy[i,0] = strategy[i-1,0] if random==False else np.random.randint(0,2)\n",
    "\n",
    "        # Agent 2's best response\n",
    "        if  reward[0,1] < reward[1,1]-epsilon:\n",
    "            strategy[i,1] = 1\n",
    "        elif reward[0,1]-epsilon > reward[1,1]:\n",
    "            strategy[i,1] = 0\n",
    "        else:\n",
    "            strategy[i,1] = strategy[i-1,1] if random==False else np.random.randint(0,2)\n",
    "\n",
    "        # update the empirical distribution\n",
    "        belief[i,0] = ((i-1)*belief[i-1,0] + strategy[i,0])/i\n",
    "        belief[i,1] = ((i-1)*belief[i-1,1] + strategy[i,1])/i\n",
    "\n",
    "        # store the (normalized) number of iterations\n",
    "        # strategy[i,2] = i/N_EPISODES\n",
    "        # belief[i,2] = i/N_EPISODES\n",
    "    return strategy, belief"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 761.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "       Agent_1  Agent_2  Agent_1_Defection  Agent_2_Defection\ncount    100.0    100.0         100.000000         100.000000\nmean       1.0      1.0           0.955960           0.951818\nstd        0.0      0.0           0.014537           0.015913\nmin        1.0      1.0           0.919192           0.919192\n25%        1.0      1.0           0.949495           0.939394\n50%        1.0      1.0           0.959596           0.949495\n75%        1.0      1.0           0.969697           0.969697\nmax        1.0      1.0           0.989899           0.989899",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Agent_1</th>\n      <th>Agent_2</th>\n      <th>Agent_1_Defection</th>\n      <th>Agent_2_Defection</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>100.0</td>\n      <td>100.0</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.955960</td>\n      <td>0.951818</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.014537</td>\n      <td>0.015913</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.919192</td>\n      <td>0.919192</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.949495</td>\n      <td>0.939394</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.959596</td>\n      <td>0.949495</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.969697</td>\n      <td>0.969697</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.989899</td>\n      <td>0.989899</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prisoner's dilemma\n",
    "# REWARD, TEMPTATION, SUCKER, PUNISHMENT = 3, 5, 0, 1\n",
    "# stag hunt\n",
    "# REWARD, TEMPTATION, SUCKER, PUNISHMENT = 4, 3, 0, 1\n",
    "REWARD, TEMPTATION, SUCKER, PUNISHMENT = 4, 4, 0, 1\n",
    "# chicken\n",
    "# REWARD, TEMPTATION, SUCKER, PUNISHMENT = 3, 4, 1, 0\n",
    "\n",
    "# C:0, D:1\n",
    "# payoff matrix for the two Agents in the 2x2 game\n",
    "payoff = np.array([[REWARD, SUCKER],[TEMPTATION, PUNISHMENT]])\n",
    "strategy_list, belief_list = [],[]\n",
    "for ep in tqdm(range(EPOCH)):\n",
    "    # 'reward' stores the payoff for each player, assuming the other player plays according to the empirical distribution\n",
    "    reward = np.zeros([2,2])\n",
    "    # 'strategy' stores the strategy played in each time period\n",
    "    strategy = -np.ones([N_EPISODES,2])\n",
    "    # 'belief' stores the empirical distribution at each time period, which is the defection probability\n",
    "    belief = np.zeros([N_EPISODES,2])\n",
    "    strategy, belief = two_agent_fictitious_play(payoff, reward, strategy, belief)\n",
    "    strategy_list.append(strategy[-1].copy())\n",
    "    belief_list.append(belief[-1].copy())\n",
    "\n",
    "df = pd.DataFrame(np.concatenate((np.array(strategy_list),np.array(belief_list)), axis=1), columns=['Agent_1', 'Agent_2', 'Agent_1_Defection', 'Agent_2_Defection'])\n",
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generalized into N-Agent Version"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def multi_agent_fictitious_play(payoff:Type.TensorType, reward:Type.TensorType, strategy:Type.TensorType, belief:Type.TensorType, N:int, alpha:float=0.05, epsilon:float=0.05, random:bool=False):\n",
    "    def random_select_partner(N:int, n:int):\n",
    "        m = n\n",
    "        while m == n:\n",
    "            m = np.random.randint(0, N)\n",
    "        return m\n",
    "\n",
    "    assert strategy.shape == (N_EPISODES, N), \"strategy columns should be N\"\n",
    "\n",
    "    log=dict()\n",
    "    # exploration\n",
    "    for i in range(1,10):\n",
    "        n = np.random.randint(0, N)\n",
    "        m = random_select_partner(N,n)\n",
    "\n",
    "        strategy[i,n] = np.random.randint(0,2)\n",
    "        strategy[i,m] = np.random.randint(0,2)\n",
    "\n",
    "        # update the empirical distribution\n",
    "        belief[n] = (1-alpha)*belief[n] + alpha*strategy[i,n]\n",
    "        belief[m] = (1-alpha)*belief[m] + alpha*strategy[i,m]\n",
    "        log[i] = {n:belief[n],m:belief[m]}\n",
    "\n",
    "    for i in range(10,N_EPISODES):\n",
    "        n = np.random.randint(0, N)\n",
    "        m = random_select_partner(N,n)\n",
    "\n",
    "        # calculate payoff of Agent 0 assuming the other Agent plays according to empirical dist.\n",
    "        reward[0,0] = belief[m]*payoff[0,1] + (1-belief[m])*payoff[0,0]\n",
    "        reward[1,0] = belief[m]*payoff[1,1] + (1-belief[m])*payoff[1,0]\n",
    "\n",
    "        # calculate payoff of Agent 1 assuming the other Agent plays according to empirical dist.\n",
    "        reward[0,1] = belief[n]*payoff[0,1] + (1-belief[n])*payoff[0,0]\n",
    "        reward[1,1] = belief[n]*payoff[1,1] + (1-belief[n])*payoff[1,0]\n",
    "\n",
    "        # Agent 1's best response\n",
    "        if reward[0,0] < reward[1,0]-epsilon:\n",
    "            strategy[i,n] = 1\n",
    "        elif reward[0,0]-epsilon > reward[1,0]:\n",
    "            strategy[i,n] = 0\n",
    "        else:\n",
    "            strategy[i,n] = np.random.randint(0,2)\n",
    "\n",
    "        # Agent 2's best response\n",
    "        if  reward[0,1] < reward[1,1]-epsilon:\n",
    "            strategy[i,m] = 1\n",
    "        elif reward[0,1]-epsilon > reward[1,1]:\n",
    "            strategy[i,m] = 0\n",
    "        else:\n",
    "            strategy[i,m] = np.random.randint(0,2)\n",
    "\n",
    "        print('error') if strategy[i,n]<0 or strategy[i,m]<0 else None\n",
    "\n",
    "        # update the empirical distribution\n",
    "        belief[n] = (1-alpha)*belief[n] + alpha*strategy[i,n]\n",
    "        belief[m] = (1-alpha)*belief[m] + alpha*strategy[i,m]\n",
    "        log[i] = {n:belief[n],m:belief[m]}\n",
    "        # store the (normalized) number of iterations\n",
    "        # strategy[i,2] = i/N_EPISODES\n",
    "        # belief[i,2] = i/N_EPISODES\n",
    "    return strategy, belief, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 374.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "          Agent_1     Agent_2     Agent_3  Agent_1_Defection  \\\ncount  100.000000  100.000000  100.000000         100.000000   \nmean    -0.290000   -0.380000   -0.330000           0.006370   \nstd      0.456048    0.487832    0.472582           0.003483   \nmin     -1.000000   -1.000000   -1.000000           0.000000   \n25%     -1.000000   -1.000000   -1.000000           0.004255   \n50%      0.000000    0.000000    0.000000           0.005946   \n75%      0.000000    0.000000    0.000000           0.007563   \nmax      0.000000    0.000000    0.000000           0.023418   \n\n       Agent_2_Defection  Agent_3_Defection  \ncount         100.000000         100.000000  \nmean            0.006026           0.006079  \nstd             0.003260           0.003270  \nmin             0.001452           0.000000  \n25%             0.003658           0.003790  \n50%             0.005356           0.006011  \n75%             0.008158           0.007938  \nmax             0.019074           0.016133  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Agent_1</th>\n      <th>Agent_2</th>\n      <th>Agent_3</th>\n      <th>Agent_1_Defection</th>\n      <th>Agent_2_Defection</th>\n      <th>Agent_3_Defection</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-0.290000</td>\n      <td>-0.380000</td>\n      <td>-0.330000</td>\n      <td>0.006370</td>\n      <td>0.006026</td>\n      <td>0.006079</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.456048</td>\n      <td>0.487832</td>\n      <td>0.472582</td>\n      <td>0.003483</td>\n      <td>0.003260</td>\n      <td>0.003270</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>0.000000</td>\n      <td>0.001452</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>0.004255</td>\n      <td>0.003658</td>\n      <td>0.003790</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.005946</td>\n      <td>0.005356</td>\n      <td>0.006011</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.007563</td>\n      <td>0.008158</td>\n      <td>0.007938</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.023418</td>\n      <td>0.019074</td>\n      <td>0.016133</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prisoner's dilemma\n",
    "# REWARD, TEMPTATION, SUCKER, PUNISHMENT = 3, 5, 0, 1\n",
    "# stag hunt\n",
    "REWARD, TEMPTATION, SUCKER, PUNISHMENT = 4, 3, 0, 1\n",
    "# REWARD, TEMPTATION, SUCKER, PUNISHMENT = 4, 4, 0, 1\n",
    "# chicken\n",
    "# REWARD, TEMPTATION, SUCKER, PUNISHMENT = 3, 4, 1, 0\n",
    "\n",
    "N = 3\n",
    "# C:0, D:1\n",
    "# payoff matrix for the two players in the 2x2 game\n",
    "payoff = np.array([[REWARD, SUCKER],[TEMPTATION, PUNISHMENT]])\n",
    "strategy_list, belief_list = [],[]\n",
    "for ep in tqdm(range(EPOCH)):\n",
    "    # 'reward' stores the payoff for each player, assuming the other player plays according to the empirical distribution\n",
    "    reward = np.zeros([2,2])\n",
    "    # 'strategy' stores the strategy played in each time period\n",
    "    strategy = -np.ones([N_EPISODES,N])\n",
    "    # 'belief' stores the empirical distribution at each time period, which is the defection probability\n",
    "    belief = np.zeros(N)\n",
    "    strategy, belief, belief_value = multi_agent_fictitious_play(payoff, reward, strategy, belief, N)\n",
    "    strategy_list.append(strategy[-1].copy())\n",
    "    belief_list.append(belief.copy())\n",
    "\n",
    "columns = []\n",
    "for i in range(N):\n",
    "    columns.append(f'Agent_{i+1}')\n",
    "for i in range(N):\n",
    "    columns.append(f'Agent_{i+1}_Defection')\n",
    "\n",
    "df = pd.DataFrame(np.concatenate((np.array(strategy_list),np.array(belief_list)), axis=1), columns=columns)\n",
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [
    {
     "data": {
      "text/plain": "{1: {1: 0.0, 0: 0.05},\n 2: {0: 0.0975, 2: 0.05},\n 3: {1: 0.0, 2: 0.0975},\n 4: {1: 0.0, 0: 0.092625},\n 5: {2: 0.142625, 1: 0.0},\n 6: {1: 0.0, 2: 0.18549375},\n 7: {1: 0.0, 0: 0.13799375},\n 8: {1: 0.05, 2: 0.2262190625},\n 9: {0: 0.18109406249999999, 2: 0.214908109375},\n 10: {0: 0.22203935937499997, 2: 0.25416270390625},\n 11: {2: 0.24145456871093748, 1: 0.0975},\n 12: {0: 0.26093739140624994, 2: 0.2793818402753906},\n 13: {2: 0.315412748261621, 1: 0.142625},\n 14: {1: 0.18549375, 0: 0.29789052183593745},\n 15: {2: 0.34964211084853997, 0: 0.33299599574414057},\n 16: {2: 0.38216000530611294, 1: 0.2262190625},\n 17: {0: 0.3663461959569335, 1: 0.264908109375},\n 18: {2: 0.41305200504080725, 1: 0.30166270390624994},\n 19: {1: 0.3365795687109374, 2: 0.4423994047887669},\n 20: {1: 0.36975059027539053, 0: 0.3980288861590868},\n 21: {1: 0.401263060761621, 0: 0.42812744185113244},\n 22: {0: 0.4567210697585758, 1: 0.4311999077235399},\n 23: {1: 0.45963991233736284, 2: 0.4702794345493285},\n 24: {1: 0.4866579167204947, 2: 0.49676546282186207},\n 25: {0: 0.48388501627064695, 2: 0.521927189680769},\n 26: {0: 0.5096907654571146, 2: 0.5458308301967305},\n 27: {2: 0.568539288686894, 0: 0.5342062271842588},\n 28: {1: 0.51232502088447, 0: 0.5574959158250459},\n 29: {0: 0.5796211200337936, 2: 0.5901123242525493},\n 30: {0: 0.600640064032104, 1: 0.5367087698402464},\n 31: {1: 0.5598733313482341, 2: 0.6106067080399219},\n 32: {0: 0.6206080608304988, 2: 0.6300763726379258},\n 33: {0: 0.6395776577889739, 2: 0.6485725540060295},\n 34: {0: 0.6575987748995252, 2: 0.666143926305728},\n 35: {0: 0.674718836154549, 1: 0.5818796647808224},\n 36: {1: 0.6027856815417814, 0: 0.6909828943468216},\n 37: {1: 0.6226463974646923, 2: 0.6828367299904416},\n 38: {1: 0.6415140775914577, 2: 0.6986948934909196},\n 39: {0: 0.7064337496294805, 1: 0.6594383737118849},\n 40: {0: 0.7211120621480065, 2: 0.7137601488163736},\n 41: {1: 0.6764664550262907, 0: 0.7350564590406062},\n 42: {0: 0.748303636088576, 1: 0.6926431322749761},\n 43: {1: 0.7080109756612273, 0: 0.7608884542841472},\n 44: {1: 0.7226104268781659, 2: 0.728072141375555},\n 45: {2: 0.7416685343067773, 1: 0.7364799055342576},\n 46: {2: 0.7545851075914384, 1: 0.7496559102575447},\n 47: {0: 0.7728440315699399, 1: 0.7621731147446675},\n 48: {2: 0.7668558522118665, 1: 0.7740644590074341},\n 49: {2: 0.7785130596012731, 0: 0.7842018299914428},\n 50: {1: 0.7853612360570624, 2: 0.7895874066212095},\n 51: {0: 0.7949917384918707, 2: 0.800108036290149},\n 52: {1: 0.7960931742542093, 0: 0.8052421515672772},\n 53: {2: 0.8101026344756416, 0: 0.8149800439889133},\n 54: {2: 0.8195975027518595, 1: 0.8062885155414988},\n 55: {0: 0.8242310417894677, 1: 0.8159740897644239},\n 56: {0: 0.8330194896999943, 1: 0.8251753852762027},\n 57: {0: 0.8413685152149946, 1: 0.8339166160123925},\n 58: {1: 0.8422207852117729, 0: 0.8493000894542448},\n 59: {2: 0.8286176276142665, 0: 0.8568350849815326},\n 60: {0: 0.863993330732456, 1: 0.8501097459511843},\n 61: {0: 0.8707936641958332, 2: 0.8371867462335532},\n 62: {0: 0.8772539809860415, 1: 0.8576042586536251},\n 63: {1: 0.8647240457209439, 2: 0.8453274089218755},\n 64: {1: 0.8714878434348967, 0: 0.8833912819367394},\n 65: {2: 0.8530610384757817, 0: 0.8892217178399024},\n 66: {1: 0.8779134512631519, 2: 0.8604079865519927},\n 67: {1: 0.8840177786999943, 2: 0.867387587224393},\n 68: {0: 0.8947606319479073, 1: 0.8898168897649946},\n 69: {1: 0.8953260452767449, 2: 0.8740182078631734},\n 70: {0: 0.900022600350512, 1: 0.9005597430129076},\n 71: {0: 0.9050214703329864, 2: 0.8803172974700147},\n 72: {0: 0.9097703968163371, 1: 0.9055317558622623},\n 73: {1: 0.9102551680691492, 2: 0.8863014325965141},\n 74: {2: 0.8919863609666884, 1: 0.9147424096656918},\n 75: {0: 0.9142818769755202, 2: 0.8973870429183539},\n 76: {0: 0.9185677831267443, 1: 0.9190052891824072},\n 77: {0: 0.9226393939704071, 2: 0.9025176907724363},\n 78: {2: 0.9073918062338144, 1: 0.9230550247232868},\n 79: {0: 0.9265074242718867, 2: 0.9120222159221237},\n 80: {1: 0.9269022734871225, 2: 0.9164211051260176},\n 81: {0: 0.9301820530582924, 2: 0.9206000498697168},\n 82: {0: 0.9336729504053778, 1: 0.9305571598127664},\n 83: {1: 0.9340293018221281, 0: 0.936989302885109},\n 84: {2: 0.9245700473762309, 1: 0.9373278367310217},\n 85: {1: 0.9404614448944706, 2: 0.9283415450074194},\n 86: {1: 0.9434383726497471, 2: 0.9319244677570484},\n 87: {0: 0.9401398377408535, 1: 0.9462664540172597},\n 88: {2: 0.935328244369196, 0: 0.9431328458538109},\n 89: {1: 0.9489531313163967, 0: 0.9459762035611203},\n 90: {0: 0.9486773933830643, 1: 0.9515054747505769},\n 91: {1: 0.953930201013048, 0: 0.9512435237139111},\n 92: {2: 0.9385618321507362, 0: 0.9536813475282155},\n 93: {2: 0.9416337405431994, 1: 0.9562336909623956},\n 94: {0: 0.9559972801518047, 1: 0.9584220064142759},\n 95: {0: 0.9581974161442145, 2: 0.9445520535160394},\n 96: {2: 0.9473244508402374, 0: 0.9602875453370038},\n 97: {2: 0.9499582282982255, 1: 0.9605009060935621},\n 98: {2: 0.9524603168833142, 0: 0.9622731680701536},\n 99: {2: 0.9548373010391485, 1: 0.962475860788884}}"
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "belief_value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}