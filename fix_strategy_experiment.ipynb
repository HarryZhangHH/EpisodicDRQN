{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from agent import *\n",
    "from selection import *\n",
    "from utils import *\n",
    "from env import Environment\n",
    "from simulation import constructAgent, twoSimulate\n",
    "from main import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything()\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN vs TfT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_convergence(agent:object, threshold:int, k:int=100):\n",
    "    if agent.play_times < 2*k:\n",
    "        return False\n",
    "    history_1 = agent.own_memory[agent.play_times-k:agent.play_times]\n",
    "    history_2 = agent.own_memory[agent.play_times-2*k:agent.play_times-k]\n",
    "    difference = torch.sum(torch.abs(history_1 - history_2))\n",
    "    if difference > threshold:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: h=1, epsilon_decay=0.95\n",
      "playing times: 20000\n",
      "length of loss: 19936, average of loss (interval is 2): 4.116038464949306, average of loss (interval is 20): 4.226298858814218, average of loss (interval is 100): 4.110161491520703\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
      "        1., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 1.])\n",
      "\n",
      "\n",
      "config: h=1, epsilon_decay=0.99\n",
      "playing times: 3000\n",
      "length of loss: 2936, average of loss (interval is 2): 7.489043057898417, average of loss (interval is 20): 8.077306690796906, average of loss (interval is 100): 5.947919905744493\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=1, epsilon_decay=0.995\n",
      "playing times: 3000\n",
      "length of loss: 2936, average of loss (interval is 2): 5.553828730759016, average of loss (interval is 20): 5.260170499005412, average of loss (interval is 100): 5.35346305022637\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=1, epsilon_decay=0.999\n",
      "playing times: 20000\n",
      "length of loss: 19936, average of loss (interval is 2): 6.9607714928180595, average of loss (interval is 20): 6.942066085807897, average of loss (interval is 100): 6.992307853475213\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=2, epsilon_decay=0.95\n",
      "playing times: 3000\n",
      "length of loss: 2935, average of loss (interval is 2): 6.633045851332909, average of loss (interval is 20): 6.171133679157237, average of loss (interval is 100): 8.51299195017976\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=2, epsilon_decay=0.99\n",
      "playing times: 3000\n",
      "length of loss: 2935, average of loss (interval is 2): 6.527573394122831, average of loss (interval is 20): 6.161011421001282, average of loss (interval is 100): 6.918588929871718\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=2, epsilon_decay=0.995\n",
      "playing times: 3000\n",
      "length of loss: 2935, average of loss (interval is 2): 5.528619367470686, average of loss (interval is 20): 4.910991740267293, average of loss (interval is 100): 4.832867745558421\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=2, epsilon_decay=0.999\n",
      "playing times: 20000\n",
      "length of loss: 19935, average of loss (interval is 2): 9.116194746826281, average of loss (interval is 20): 9.202611119182802, average of loss (interval is 100): 9.1578649610281\n",
      "==================================================\n",
      "Your action: tensor([1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n",
      "Oppo action: tensor([0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0.])\n",
      "\n",
      "\n",
      "config: h=5, epsilon_decay=0.95\n",
      "playing times: 5000\n",
      "length of loss: 4932, average of loss (interval is 2): 5.238831492595598, average of loss (interval is 20): 5.181681190425085, average of loss (interval is 100): 4.975665811300278\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=5, epsilon_decay=0.99\n",
      "playing times: 20000\n",
      "length of loss: 19932, average of loss (interval is 2): 6.124385459983645, average of loss (interval is 20): 5.662257090360375, average of loss (interval is 100): 5.5930304464697835\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=5, epsilon_decay=0.995\n",
      "playing times: 20000\n",
      "length of loss: 19932, average of loss (interval is 2): 3.335725864022073, average of loss (interval is 20): 3.109419441629676, average of loss (interval is 100): 3.0852085129916667\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 1.])\n",
      "\n",
      "\n",
      "config: h=5, epsilon_decay=0.999\n",
      "playing times: 20000\n",
      "length of loss: 19932, average of loss (interval is 2): 5.808511346349085, average of loss (interval is 20): 5.735954704396464, average of loss (interval is 100): 5.864915937930346\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=10, epsilon_decay=0.95\n",
      "playing times: 5000\n",
      "length of loss: 4927, average of loss (interval is 2): 68.8564478519929, average of loss (interval is 20): 70.23284672025727, average of loss (interval is 100): 71.70027482509613\n",
      "==================================================\n",
      "Your action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "Oppo action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "\n",
      "\n",
      "config: h=10, epsilon_decay=0.99\n",
      "playing times: 4000\n",
      "length of loss: 3927, average of loss (interval is 2): 80.01418668784338, average of loss (interval is 20): 82.60251511928394, average of loss (interval is 100): 89.6250190347433\n",
      "==================================================\n",
      "Your action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "Oppo action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "\n",
      "\n",
      "config: h=10, epsilon_decay=0.995\n",
      "playing times: 10000\n",
      "length of loss: 9927, average of loss (interval is 2): 13.81282699755331, average of loss (interval is 20): 13.545177028182767, average of loss (interval is 100): 13.103761556148529\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=10, epsilon_decay=0.999\n",
      "playing times: 20000\n",
      "length of loss: 19927, average of loss (interval is 2): 5.89380206365403, average of loss (interval is 20): 6.18386113954523, average of loss (interval is 100): 6.109332359731197\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# choices = {'0-alwaysCooperate','1-alwaysDefect','2-titForTat','3-reverseTitForTat','4-random','5-grudger','6-pavlov','7-qLearning','8-lstm-TFT','9-dqn','10-lstmqn','11-a2c','12-a2c-lstm'}\n",
    "# rl_choices = {'7-qLearning','8-lstm-pavlov','9-dqn','10-lstmqn','11-a2c','12-a2c-lstm'}\n",
    "strategies = {0:'ALLC',1:'ALLD',2:'TitForTat',3:'revTitForTat',4:'Random',5:'Grudger',6:'Pavlov',7:'QLearning',8:'LSTM',9:'DQN',10:'LSTMQN',11:'A2C',12:'A2CLSTM'}\n",
    "\n",
    "h = [1,2,5,10]\n",
    "epsilon_decay = [0.95, 0.99, 0.995, 0.999]\n",
    "\n",
    "config = {\n",
    "    'reward': 3, \n",
    "    'sucker': 0, \n",
    "    'temptation': 5, \n",
    "    'punishment': 1, \n",
    "    'n_episodes': 10000, \n",
    "    'discount': 0.99,\n",
    "    'play_epsilon': 1,\n",
    "    'select_epsilon': 1,\n",
    "    'epsilon_decay': 0.999,\n",
    "    'min_epsilon': 0.01,\n",
    "    'alpha': 0.1,\n",
    "    'n_actions': 2,\n",
    "    'h': 10,\n",
    "    'state_repr': 'bi',\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 1e-3,\n",
    "}\n",
    "\n",
    "epsilon_dict = {'epsilon_decay=0.95':[],'epsilon_decay=0.99':[],'epsilon_decay=0.995':[],'epsilon_decay=0.999':[]}\n",
    "result_dict={'h=1':{}, 'h=2':{}, 'h=5':{}, 'h=10':{}}\n",
    "loss_dict={'h=1':{}, 'h=2':{}, 'h=5':{}, 'h=10':{}}\n",
    "strategy_dict={'h=1':{}, 'h=2':{}, 'h=5':{}, 'h=10':{}}\n",
    "                \n",
    "for key in result_dict:\n",
    "    result_dict[key] = copy.deepcopy(epsilon_dict)\n",
    "    loss_dict[key] = copy.deepcopy(epsilon_dict)\n",
    "    strategy_dict[key] = copy.deepcopy(epsilon_dict)\n",
    "\n",
    "for i in h:\n",
    "    for j in epsilon_decay:\n",
    "        config['h'] = i\n",
    "        config['epsilon_decay'] = j\n",
    "        config_ob = Config(config)\n",
    "        env = Environment(config_ob)\n",
    "        print(f'config: h={i}, epsilon_decay={j}')\n",
    "        \n",
    "        num = 2\n",
    "        rl_num = 9\n",
    "        for _ in range(5):\n",
    "            convergence = False\n",
    "            # twoSimulate(dict({num: strategies[num],rl_num: strategies[rl_num]}), rl_num, config)\n",
    "            with HiddenPrints():\n",
    "                agent1 = constructAgent(strategies[num], config_ob)\n",
    "                agent2 = constructAgent(strategies[rl_num], config_ob)\n",
    "\n",
    "                k = 1000\n",
    "                while not convergence:\n",
    "                    env.play(agent1, agent2, k)\n",
    "                    convergence = determine_convergence(agent2, 20, k=k)\n",
    "                    if agent2.play_times >= 20*k:\n",
    "                        break\n",
    "        \n",
    "                result_dict[f'h={i}'][f'epsilon_decay={j}'].append(agent2.play_times)\n",
    "                loss_dict[f'h={i}'][f'epsilon_decay={j}'].append(np.mean(agent2.loss))\n",
    "                strategy_dict[f'h={i}'][f'epsilon_decay={j}'].append(list(agent2.own_memory[agent2.play_times-10:agent2.play_times]))\n",
    "        print(f'playing times: {agent2.play_times}')\n",
    "        print(f'length of loss: {len(agent2.loss)}, average of loss (interval is 2): {np.mean(agent2.loss[::2])}, average of loss (interval is 20): {np.mean(agent2.loss[::20])}, average of loss (interval is 100): {np.mean(agent2.loss[::100])}')\n",
    "        agent2.show()\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "    # plt.plot(agent2.loss[::20])\n",
    "    # plt.title(f'agent:{agent2.name}')\n",
    "    # plt.show()\n",
    "# agent1.show()\n",
    "# agent2.show()\n",
    "# print(\"==================================================\")\n",
    "# print(f'{agent1.name} score: {agent1.running_score}\\n{agent2.name} score: {agent2.running_score}')\n",
    "# print(\"------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "# print()\n",
    "\n",
    "# x = [i for i in range(0, agent1.play_times)]\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# plt.plot(x, agent1.own_memory[0:agent1.play_times], label=agent1.name, alpha=0.5)\n",
    "# plt.plot(x, agent2.own_memory[0:agent2.play_times], label=agent2.name, alpha=0.5)\n",
    "# plt.legend()\n",
    "# plt.ylim(-0.5, 2)\n",
    "# plt.xlim(0, agent1.play_times)\n",
    "# plt.title(f'agent:{agent1.name} vs agent:{agent2.name}')\n",
    "# plt.savefig(f'images/{agent1.name}vs{agent2.name}_result_h={config.h}.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h=1': {'epsilon_decay=0.95': [3000, 20000, 3000, 3000, 20000], 'epsilon_decay=0.99': [3000, 3000, 3000, 3000, 3000], 'epsilon_decay=0.995': [3000, 3000, 3000, 3000, 3000], 'epsilon_decay=0.999': [20000, 20000, 20000, 20000, 20000]}, 'h=2': {'epsilon_decay=0.95': [4000, 3000, 5000, 8000, 3000], 'epsilon_decay=0.99': [8000, 3000, 4000, 3000, 3000], 'epsilon_decay=0.995': [3000, 4000, 9000, 4000, 3000], 'epsilon_decay=0.999': [20000, 20000, 20000, 20000, 20000]}, 'h=5': {'epsilon_decay=0.95': [3000, 3000, 7000, 4000, 5000], 'epsilon_decay=0.99': [3000, 3000, 5000, 8000, 20000], 'epsilon_decay=0.995': [7000, 20000, 6000, 4000, 20000], 'epsilon_decay=0.999': [20000, 9000, 20000, 20000, 20000]}, 'h=10': {'epsilon_decay=0.95': [20000, 3000, 3000, 3000, 5000], 'epsilon_decay=0.99': [5000, 6000, 20000, 3000, 4000], 'epsilon_decay=0.995': [4000, 4000, 3000, 4000, 10000], 'epsilon_decay=0.999': [20000, 20000, 20000, 20000, 20000]}}\n",
      "\n",
      "{'h=1': {'epsilon_decay=0.95': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 1, 0, 1, 0, 1, 0]], 'epsilon_decay=0.99': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'epsilon_decay=0.995': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'epsilon_decay=0.999': [[0, 1, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}, 'h=2': {'epsilon_decay=0.95': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'epsilon_decay=0.99': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'epsilon_decay=0.995': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'epsilon_decay=0.999': [[0, 1, 0, 1, 0, 1, 0, 1, 0, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 1, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]}, 'h=5': {'epsilon_decay=0.95': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'epsilon_decay=0.99': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'epsilon_decay=0.995': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 1, 0, 0, 0, 1, 0]], 'epsilon_decay=0.999': [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}, 'h=10': {'epsilon_decay=0.95': [[1, 0, 1, 0, 0, 0, 1, 0, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'epsilon_decay=0.99': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'epsilon_decay=0.995': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'epsilon_decay=0.999': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 1, 1, 1, 1, 0, 1], [0, 1, 0, 0, 1, 1, 0, 1, 1, 1], [0, 0, 1, 0, 0, 0, 1, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}}\n"
     ]
    }
   ],
   "source": [
    "for i in h:\n",
    "    for j in epsilon_decay:\n",
    "        strategy_dict[f'h={i}'][f'epsilon_decay={j}'] = torch.Tensor(strategy_dict[f'h={i}'][f'epsilon_decay={j}']).numpy().astype(int).tolist()\n",
    "print(result_dict)\n",
    "print()\n",
    "print(strategy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.8020073184537395\n",
      "7.096781128811779\n",
      "6.426352696914155\n",
      "7.009957177002221\n",
      "9.312634296046323\n",
      "7.32161069686447\n",
      "6.298518353351357\n",
      "7.8245764660819574\n",
      "7.374517959213087\n",
      "13.117284897510364\n",
      "22.171735876204444\n",
      "6.650255216491738\n",
      "41.68496656525413\n",
      "54.37895088756359\n",
      "75.1673582437496\n",
      "5.0361007256933545\n"
     ]
    }
   ],
   "source": [
    "for i in h:\n",
    "    for j in epsilon_decay:\n",
    "        print(np.mean(loss_dict[f'h={i}'][f'epsilon_decay={j}']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN vs DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: h=1, epsilon_decay=0.95\n",
      "playing times: 6000\n",
      "length of loss: 5936, average of loss (interval is 2): 16.66003415278756, average of loss (interval is 20): 16.415073569776233, average of loss (interval is 100): 15.694142721096675\n",
      "length of loss: 5936, average of loss (interval is 2): 7.205223576020138, average of loss (interval is 20): 7.050136550257503, average of loss (interval is 100): 7.05993527273337\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=1, epsilon_decay=0.99\n",
      "playing times: 20000\n",
      "length of loss: 19936, average of loss (interval is 2): 3.765200049057304, average of loss (interval is 20): 3.766812394022822, average of loss (interval is 100): 3.842576918378472\n",
      "length of loss: 19936, average of loss (interval is 2): 8.724930017325706, average of loss (interval is 20): 8.81258524953058, average of loss (interval is 100): 8.92036146134138\n",
      "==================================================\n",
      "Your action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=1, epsilon_decay=0.995\n",
      "playing times: 16000\n",
      "length of loss: 15936, average of loss (interval is 2): 4.520882575903962, average of loss (interval is 20): 4.374750629696093, average of loss (interval is 100): 4.067906654532999\n",
      "length of loss: 15936, average of loss (interval is 2): 5.612996385374762, average of loss (interval is 20): 5.638637574162881, average of loss (interval is 100): 5.772265244089067\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=1, epsilon_decay=0.999\n",
      "playing times: 20000\n",
      "length of loss: 19936, average of loss (interval is 2): 6.8469775897076275, average of loss (interval is 20): 6.954814254295981, average of loss (interval is 100): 6.88328926205635\n",
      "length of loss: 19936, average of loss (interval is 2): 7.188756552878035, average of loss (interval is 20): 7.393612552191334, average of loss (interval is 100): 7.385116039812565\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 0.])\n",
      "Oppo action: tensor([0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 1.])\n",
      "\n",
      "\n",
      "config: h=2, epsilon_decay=0.95\n",
      "playing times: 3000\n",
      "length of loss: 2935, average of loss (interval is 2): 17.55214894240169, average of loss (interval is 20): 17.566902163482847, average of loss (interval is 100): 20.068946458896\n",
      "length of loss: 2935, average of loss (interval is 2): 21.0076778739975, average of loss (interval is 20): 21.324132505728272, average of loss (interval is 100): 20.004488543669382\n",
      "==================================================\n",
      "Your action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=2, epsilon_decay=0.99\n",
      "playing times: 11000\n",
      "length of loss: 10935, average of loss (interval is 2): 13.558629614031917, average of loss (interval is 20): 13.785687544337138, average of loss (interval is 100): 13.124650805104862\n",
      "length of loss: 10935, average of loss (interval is 2): 26.401980486581948, average of loss (interval is 20): 25.470043165592216, average of loss (interval is 100): 26.0094155495817\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 1.])\n",
      "Oppo action: tensor([0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "\n",
      "\n",
      "config: h=2, epsilon_decay=0.995\n",
      "playing times: 6000\n",
      "length of loss: 5935, average of loss (interval is 2): 42.85150478089434, average of loss (interval is 20): 44.321319023768105, average of loss (interval is 100): 46.21225399573644\n",
      "length of loss: 5935, average of loss (interval is 2): 40.158575284513184, average of loss (interval is 20): 39.308419002426994, average of loss (interval is 100): 39.07668347160021\n",
      "==================================================\n",
      "Your action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "Oppo action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "\n",
      "\n",
      "config: h=2, epsilon_decay=0.999\n",
      "playing times: 20000\n",
      "length of loss: 19935, average of loss (interval is 2): 10.411893473331178, average of loss (interval is 20): 10.481600661933003, average of loss (interval is 100): 10.41362331032753\n",
      "length of loss: 19935, average of loss (interval is 2): 12.675980270375314, average of loss (interval is 20): 12.655938820351091, average of loss (interval is 100): 13.127351343035699\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "\n",
      "\n",
      "config: h=5, epsilon_decay=0.95\n",
      "playing times: 5000\n",
      "length of loss: 4932, average of loss (interval is 2): 70.08712882358228, average of loss (interval is 20): 69.48771346858156, average of loss (interval is 100): 70.90899972438812\n",
      "length of loss: 4932, average of loss (interval is 2): 64.01869774670836, average of loss (interval is 20): 63.16924914066126, average of loss (interval is 100): 60.436528687477114\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "        1., 0.])\n",
      "\n",
      "\n",
      "config: h=5, epsilon_decay=0.99\n",
      "playing times: 9000\n",
      "length of loss: 8932, average of loss (interval is 2): 39.96534475490645, average of loss (interval is 20): 38.717456860963665, average of loss (interval is 100): 39.44904682901171\n",
      "length of loss: 8932, average of loss (interval is 2): 37.25934333912444, average of loss (interval is 20): 36.3895266880125, average of loss (interval is 100): 36.44980747699738\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=5, epsilon_decay=0.995\n",
      "playing times: 9000\n",
      "length of loss: 8932, average of loss (interval is 2): 54.68528101540872, average of loss (interval is 20): 54.039261191216625, average of loss (interval is 100): 55.8409942706426\n",
      "length of loss: 8932, average of loss (interval is 2): 70.80830395387204, average of loss (interval is 20): 67.75712728713717, average of loss (interval is 100): 67.40715488990148\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 1.])\n",
      "\n",
      "\n",
      "config: h=5, epsilon_decay=0.999\n",
      "playing times: 6000\n",
      "length of loss: 5932, average of loss (interval is 2): 30.394955691339515, average of loss (interval is 20): 28.87201431784967, average of loss (interval is 100): 33.41156587600708\n",
      "length of loss: 5932, average of loss (interval is 2): 36.183091324202046, average of loss (interval is 20): 36.21329724307012, average of loss (interval is 100): 39.54559892018636\n",
      "==================================================\n",
      "Your action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "Oppo action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "\n",
      "\n",
      "config: h=10, epsilon_decay=0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing times: 4000\n",
      "length of loss: 3927, average of loss (interval is 2): 39.74156008666863, average of loss (interval is 20): 39.14970471350675, average of loss (interval is 100): 38.4372915238142\n",
      "length of loss: 3927, average of loss (interval is 2): 53.9493605391314, average of loss (interval is 20): 55.327261114483555, average of loss (interval is 100): 57.51042265892029\n",
      "==================================================\n",
      "Your action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "Oppo action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "\n",
      "\n",
      "config: h=10, epsilon_decay=0.99\n",
      "playing times: 4000\n",
      "length of loss: 3927, average of loss (interval is 2): 92.06164374024951, average of loss (interval is 20): 92.61497359106383, average of loss (interval is 100): 90.12103570997715\n",
      "length of loss: 3927, average of loss (interval is 2): 83.13210276840539, average of loss (interval is 20): 84.06750825667744, average of loss (interval is 100): 88.44111963957548\n",
      "==================================================\n",
      "Your action: tensor([1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=10, epsilon_decay=0.995\n",
      "playing times: 3000\n",
      "length of loss: 2927, average of loss (interval is 2): 8.895647543110972, average of loss (interval is 20): 8.479633167487423, average of loss (interval is 100): 8.834780081113179\n",
      "length of loss: 2927, average of loss (interval is 2): 21.05028306053636, average of loss (interval is 20): 23.034075151495383, average of loss (interval is 100): 24.910581290721893\n",
      "==================================================\n",
      "Your action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=10, epsilon_decay=0.999\n",
      "playing times: 9000\n",
      "length of loss: 8927, average of loss (interval is 2): 26.219023888576842, average of loss (interval is 20): 26.38919872405545, average of loss (interval is 100): 26.48603355884552\n",
      "length of loss: 8927, average of loss (interval is 2): 25.209809399834135, average of loss (interval is 20): 25.508031023428742, average of loss (interval is 100): 26.493564511670005\n",
      "==================================================\n",
      "Your action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "Oppo action: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict_dqn={'h=1':{}, 'h=2':{}, 'h=5':{}, 'h=10':{}}\n",
    "loss_dict_dqn={'h=1':{}, 'h=2':{}, 'h=5':{}, 'h=10':{}}\n",
    "strategy_dict_dqn={'h=1':{}, 'h=2':{}, 'h=5':{}, 'h=10':{}}\n",
    "\n",
    "for key in result_dict_dqn:\n",
    "    result_dict_dqn[key] = copy.deepcopy(epsilon_dict)\n",
    "    loss_dict_dqn[key] = copy.deepcopy(epsilon_dict)\n",
    "    strategy_dict_dqn[key] = copy.deepcopy(epsilon_dict)\n",
    "\n",
    "for i in h:\n",
    "    for j in epsilon_decay:\n",
    "        config['h'] = i\n",
    "        config['epsilon_decay'] = j\n",
    "        config_ob = Config(config)\n",
    "        env = Environment(config_ob)\n",
    "        print(f'config: h={i}, epsilon_decay={j}')\n",
    "        \n",
    "        rl_num = 9\n",
    "        for _ in range(5):\n",
    "            convergence = False\n",
    "            with HiddenPrints():\n",
    "                agent1 = constructAgent(strategies[rl_num], config_ob)\n",
    "                agent2 = constructAgent(strategies[rl_num], config_ob)\n",
    "\n",
    "                k = 1000\n",
    "                while not convergence:\n",
    "                    env.play(agent1, agent2, k)\n",
    "                    convergence = determine_convergence(agent2, 20, k=k)\n",
    "                    if agent2.play_times >= 20*k:\n",
    "                        break\n",
    "        \n",
    "                result_dict_dqn[f'h={i}'][f'epsilon_decay={j}'].append(agent2.play_times)\n",
    "                loss_dict_dqn[f'h={i}'][f'epsilon_decay={j}'].append([np.mean(agent1.loss),np.mean(agent2.loss)])\n",
    "                strategy_dict_dqn[f'h={i}'][f'epsilon_decay={j}'].append([agent1.own_memory[agent1.play_times-10:agent1.play_times].numpy().astype(int).tolist(), agent2.own_memory[agent2.play_times-10:agent2.play_times].numpy().astype(int).tolist()])\n",
    "        print(f'playing times: {agent2.play_times}')\n",
    "        print(f'length of loss: {len(agent1.loss)}, average of loss (interval is 2): {np.mean(agent1.loss[::2])}, average of loss (interval is 20): {np.mean(agent1.loss[::20])}, average of loss (interval is 100): {np.mean(agent1.loss[::100])}')\n",
    "        print(f'length of loss: {len(agent2.loss)}, average of loss (interval is 2): {np.mean(agent2.loss[::2])}, average of loss (interval is 20): {np.mean(agent2.loss[::20])}, average of loss (interval is 100): {np.mean(agent2.loss[::100])}')\n",
    "        agent2.show()\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h=1': {'epsilon_decay=0.95': [20000, 20000, 3000, 3000, 6000],\n",
       "  'epsilon_decay=0.99': [20000, 20000, 3000, 20000, 20000],\n",
       "  'epsilon_decay=0.995': [4000, 20000, 20000, 8000, 16000],\n",
       "  'epsilon_decay=0.999': [20000, 20000, 20000, 20000, 20000]},\n",
       " 'h=2': {'epsilon_decay=0.95': [3000, 3000, 2000, 3000, 3000],\n",
       "  'epsilon_decay=0.99': [20000, 4000, 8000, 3000, 11000],\n",
       "  'epsilon_decay=0.995': [4000, 20000, 18000, 5000, 6000],\n",
       "  'epsilon_decay=0.999': [20000, 20000, 20000, 20000, 20000]},\n",
       " 'h=5': {'epsilon_decay=0.95': [20000, 8000, 3000, 5000, 5000],\n",
       "  'epsilon_decay=0.99': [3000, 6000, 8000, 4000, 9000],\n",
       "  'epsilon_decay=0.995': [20000, 8000, 8000, 20000, 9000],\n",
       "  'epsilon_decay=0.999': [20000, 7000, 20000, 20000, 6000]},\n",
       " 'h=10': {'epsilon_decay=0.95': [3000, 4000, 10000, 20000, 4000],\n",
       "  'epsilon_decay=0.99': [3000, 5000, 19000, 4000, 4000],\n",
       "  'epsilon_decay=0.995': [6000, 5000, 3000, 3000, 3000],\n",
       "  'epsilon_decay=0.999': [20000, 6000, 9000, 15000, 9000]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict_dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h=1': {'epsilon_decay=0.95': [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "    [1, 1, 0, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]],\n",
       "  'epsilon_decay=0.99': [[[1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
       "  'epsilon_decay=0.995': [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[1, 0, 1, 0, 1, 0, 1, 0, 1, 0], [1, 0, 1, 0, 1, 0, 1, 1, 1, 1]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]],\n",
       "  'epsilon_decay=0.999': [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "    [1, 1, 0, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[1, 0, 0, 1, 0, 0, 1, 0, 0, 1], [1, 0, 1, 1, 0, 1, 1, 0, 1, 1]],\n",
       "   [[1, 1, 0, 1, 1, 0, 1, 1, 0, 1], [0, 1, 0, 0, 1, 0, 0, 1, 0, 0]]]},\n",
       " 'h=2': {'epsilon_decay=0.95': [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
       "  'epsilon_decay=0.99': [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   [[0, 1, 0, 1, 0, 1, 0, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 0, 0, 1, 0, 0, 1, 0, 0, 1]]],\n",
       "  'epsilon_decay=0.995': [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   [[1, 1, 0, 0, 1, 1, 0, 0, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[1, 0, 1, 1, 1, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
       "  'epsilon_decay=0.999': [[[1, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
       "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[1, 1, 0, 1, 1, 0, 1, 1, 0, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]},\n",
       " 'h=5': {'epsilon_decay=0.95': [[[1, 1, 1, 1, 1, 0, 1, 0, 1, 0],\n",
       "    [0, 1, 0, 0, 0, 0, 1, 0, 0, 0]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[0, 1, 1, 0, 1, 1, 0, 1, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]],\n",
       "  'epsilon_decay=0.99': [[[1, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 1, 1, 1]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]],\n",
       "  'epsilon_decay=0.995': [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[0, 0, 0, 1, 1, 0, 0, 0, 1, 1], [1, 1, 1, 0, 0, 1, 1, 1, 0, 0]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]],\n",
       "  'epsilon_decay=0.999': [[[0, 0, 1, 1, 1, 1, 1, 0, 1, 1],\n",
       "    [0, 1, 1, 1, 0, 0, 0, 0, 0, 0]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 1, 0, 0, 1, 0, 0, 1, 0]],\n",
       "   [[1, 0, 0, 1, 0, 0, 0, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]},\n",
       " 'h=10': {'epsilon_decay=0.95': [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 1, 1, 1, 1, 0]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
       "  'epsilon_decay=0.99': [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 0, 1, 1, 0, 1, 1, 1, 1]],\n",
       "   [[0, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
       "  'epsilon_decay=0.995': [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[1, 1, 1, 1, 1, 1, 1, 1, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]],\n",
       "  'epsilon_decay=0.999': [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "   [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy_dict_dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.36281814012781\n",
      "5.30305792075878\n",
      "8.368344093669322\n",
      "6.437139419724933\n",
      "16.61400641145889\n",
      "22.313856821682563\n",
      "25.244923164907213\n",
      "9.55751421001342\n",
      "33.46241106023884\n",
      "36.018553720553626\n",
      "22.911297939896606\n",
      "33.22744478771189\n",
      "34.79207039937955\n",
      "68.35792606106725\n",
      "41.8682668179988\n",
      "41.587205091281305\n"
     ]
    }
   ],
   "source": [
    "for i in h:\n",
    "    for j in epsilon_decay:\n",
    "        print(np.mean(np.array(loss_dict_dqn[f'h={i}'][f'epsilon_decay={j}'])[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRQN vs TfT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: h=1, epsilon_decay=0.95\n",
      "playing times: 3000\n",
      "length of loss: 2936, average of loss (interval is 2): 0.10015642578853146, average of loss (interval is 20): 0.11335388142257823, average of loss (interval is 100): 0.09106246220738588\n",
      "==================================================\n",
      "Your action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "Oppo action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "\n",
      "\n",
      "config: h=1, epsilon_decay=0.99\n",
      "playing times: 3000\n",
      "length of loss: 2936, average of loss (interval is 2): 0.33327397430011857, average of loss (interval is 20): 0.49396941360202223, average of loss (interval is 100): 0.19131687412736936\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=1, epsilon_decay=0.995\n",
      "playing times: 3000\n",
      "length of loss: 2936, average of loss (interval is 2): 0.3266447026405703, average of loss (interval is 20): 0.49501647736984883, average of loss (interval is 100): 0.1768750812082241\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=1, epsilon_decay=0.999\n",
      "playing times: 5000\n",
      "length of loss: 4936, average of loss (interval is 2): 0.25168615774331105, average of loss (interval is 20): 0.4106800219483946, average of loss (interval is 100): 0.08913620308274403\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=2, epsilon_decay=0.95\n",
      "playing times: 9000\n",
      "length of loss: 8935, average of loss (interval is 2): 0.18491797373476765, average of loss (interval is 20): 0.283287303955637, average of loss (interval is 100): 0.07406892136803234\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=2, epsilon_decay=0.99\n",
      "playing times: 3000\n",
      "length of loss: 2935, average of loss (interval is 2): 0.30009493019162264, average of loss (interval is 20): 0.3944752398289033, average of loss (interval is 100): 0.17700867008073448\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n",
      "\n",
      "\n",
      "config: h=2, epsilon_decay=0.995\n",
      "playing times: 3000\n",
      "length of loss: 2935, average of loss (interval is 2): 0.29532466753649017, average of loss (interval is 20): 0.4025108070365394, average of loss (interval is 100): 0.1648667334365503\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=2, epsilon_decay=0.999\n",
      "playing times: 6000\n",
      "length of loss: 5935, average of loss (interval is 2): 0.18778196186391485, average of loss (interval is 20): 0.2772462315567928, average of loss (interval is 100): 0.06200945428281557\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=5, epsilon_decay=0.95\n",
      "playing times: 3000\n",
      "length of loss: 2932, average of loss (interval is 2): 0.0834997284214843, average of loss (interval is 20): 0.059457252449685145, average of loss (interval is 100): 0.07033992432160024\n",
      "==================================================\n",
      "Your action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "Oppo action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "\n",
      "\n",
      "config: h=5, epsilon_decay=0.99\n",
      "playing times: 3000\n",
      "length of loss: 2932, average of loss (interval is 2): 0.3966862368641721, average of loss (interval is 20): 0.43946657327447597, average of loss (interval is 100): 0.2564066491295913\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=5, epsilon_decay=0.995\n",
      "playing times: 3000\n",
      "length of loss: 2932, average of loss (interval is 2): 0.4886926507827298, average of loss (interval is 20): 0.4915750823041253, average of loss (interval is 100): 0.3549771409874666\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=5, epsilon_decay=0.999\n",
      "playing times: 6000\n",
      "length of loss: 5932, average of loss (interval is 2): 0.21635857638234463, average of loss (interval is 20): 0.16582668665192027, average of loss (interval is 100): 0.08200439325786041\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=10, epsilon_decay=0.95\n",
      "playing times: 3000\n",
      "length of loss: 2927, average of loss (interval is 2): 0.30423998141325265, average of loss (interval is 20): 0.1959365050423799, average of loss (interval is 100): 0.15178332790336146\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n",
      "\n",
      "\n",
      "config: h=10, epsilon_decay=0.99\n",
      "playing times: 3000\n",
      "length of loss: 2927, average of loss (interval is 2): 0.3790298603827732, average of loss (interval is 20): 0.22312680615839575, average of loss (interval is 100): 0.2317217249784638\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=10, epsilon_decay=0.995\n",
      "playing times: 3000\n",
      "length of loss: 2927, average of loss (interval is 2): 0.35119796668292513, average of loss (interval is 20): 0.26231005321050654, average of loss (interval is 100): 0.23802714115784815\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n",
      "config: h=10, epsilon_decay=0.999\n",
      "playing times: 6000\n",
      "length of loss: 5927, average of loss (interval is 2): 0.22773697855520267, average of loss (interval is 20): 0.15110759103520738, average of loss (interval is 100): 0.10903723894007271\n",
      "==================================================\n",
      "Your action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Oppo action: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict_drqn_t={'h=1':{}, 'h=2':{}, 'h=5':{}, 'h=10':{}}\n",
    "loss_dict_drqn_t={'h=1':{}, 'h=2':{}, 'h=5':{}, 'h=10':{}}\n",
    "strategy_dict_drqn_t={'h=1':{}, 'h=2':{}, 'h=5':{}, 'h=10':{}}\n",
    "\n",
    "for key in result_dict_drqn_t:\n",
    "    result_dict_drqn_t[key] = copy.deepcopy(epsilon_dict)\n",
    "    loss_dict_drqn_t[key] = copy.deepcopy(epsilon_dict)\n",
    "    strategy_dict_drqn_t[key] = copy.deepcopy(epsilon_dict)\n",
    "\n",
    "for i in h:\n",
    "    for j in epsilon_decay:\n",
    "        config['h'] = i\n",
    "        config['epsilon_decay'] = j\n",
    "        config_ob = Config(config)\n",
    "        env = Environment(config_ob)\n",
    "        print(f'config: h={i}, epsilon_decay={j}')\n",
    "        \n",
    "        n_num = 2\n",
    "        rl_num = 10\n",
    "        for _ in range(5):\n",
    "            convergence = False\n",
    "            with HiddenPrints():\n",
    "                agent1 = constructAgent(strategies[n_num], config_ob)\n",
    "                agent2 = constructAgent(strategies[rl_num], config_ob)\n",
    "\n",
    "                k = 1000\n",
    "                while not convergence:\n",
    "                    env.play(agent1, agent2, k)\n",
    "                    convergence = determine_convergence(agent2, 20, k=k)\n",
    "                    if agent2.play_times >= 20*k:\n",
    "                        break\n",
    "        \n",
    "                result_dict_drqn_t[f'h={i}'][f'epsilon_decay={j}'].append(agent2.play_times)\n",
    "                loss_dict_drqn_t[f'h={i}'][f'epsilon_decay={j}'].append(np.mean(agent2.loss))\n",
    "                strategy_dict_drqn_t[f'h={i}'][f'epsilon_decay={j}'].append(agent2.own_memory[agent2.play_times-10:agent2.play_times].numpy().astype(int).tolist())\n",
    "        print(f'playing times: {agent2.play_times}')\n",
    "        print(f'length of loss: {len(agent2.loss)}, average of loss (interval is 2): {np.mean(agent2.loss[::2])}, average of loss (interval is 20): {np.mean(agent2.loss[::20])}, average of loss (interval is 100): {np.mean(agent2.loss[::100])}')\n",
    "        agent2.show()\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h=1': {'epsilon_decay=0.95': [3000, 3000, 3000, 2000, 3000],\n",
       "  'epsilon_decay=0.99': [3000, 3000, 3000, 3000, 3000],\n",
       "  'epsilon_decay=0.995': [3000, 3000, 3000, 3000, 3000],\n",
       "  'epsilon_decay=0.999': [6000, 5000, 5000, 6000, 5000]},\n",
       " 'h=2': {'epsilon_decay=0.95': [4000, 2000, 5000, 2000, 9000],\n",
       "  'epsilon_decay=0.99': [3000, 3000, 3000, 3000, 3000],\n",
       "  'epsilon_decay=0.995': [3000, 3000, 3000, 3000, 3000],\n",
       "  'epsilon_decay=0.999': [5000, 5000, 5000, 8000, 6000]},\n",
       " 'h=5': {'epsilon_decay=0.95': [3000, 13000, 2000, 4000, 3000],\n",
       "  'epsilon_decay=0.99': [3000, 3000, 3000, 3000, 3000],\n",
       "  'epsilon_decay=0.995': [3000, 3000, 3000, 6000, 3000],\n",
       "  'epsilon_decay=0.999': [11000, 9000, 5000, 6000, 6000]},\n",
       " 'h=10': {'epsilon_decay=0.95': [3000, 4000, 2000, 3000, 3000],\n",
       "  'epsilon_decay=0.99': [3000, 3000, 3000, 3000, 3000],\n",
       "  'epsilon_decay=0.995': [3000, 3000, 3000, 3000, 3000],\n",
       "  'epsilon_decay=0.999': [10000, 5000, 8000, 9000, 6000]}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict_drqn_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h=1': {'epsilon_decay=0.95': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "  'epsilon_decay=0.99': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "  'epsilon_decay=0.995': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "  'epsilon_decay=0.999': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]},\n",
       " 'h=2': {'epsilon_decay=0.95': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "  'epsilon_decay=0.99': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "  'epsilon_decay=0.995': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "  'epsilon_decay=0.999': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]},\n",
       " 'h=5': {'epsilon_decay=0.95': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "  'epsilon_decay=0.99': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [1, 0, 1, 1, 1, 1, 1, 1, 0, 1],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "  'epsilon_decay=0.995': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "  'epsilon_decay=0.999': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]},\n",
       " 'h=10': {'epsilon_decay=0.95': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "   [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "  'epsilon_decay=0.99': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "  'epsilon_decay=0.995': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "  'epsilon_decay=0.999': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy_dict_drqn_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19736206660347236\n",
      "0.3169081791133227\n",
      "0.3141808840314436\n",
      "0.2345171351167666\n",
      "0.1611805972969937\n",
      "0.31566605395002834\n",
      "0.29898199706499035\n",
      "0.20585462610902844\n",
      "0.13222151485549857\n",
      "0.3299996406784215\n",
      "0.388512719779985\n",
      "0.2090834334060912\n",
      "0.20063491137410666\n",
      "0.3059607806440088\n",
      "0.37549847163298866\n",
      "0.2417215784707242\n"
     ]
    }
   ],
   "source": [
    "for i in h:\n",
    "    for j in epsilon_decay:\n",
    "        print(np.mean(loss_dict_drqn_t[f'h={i}'][f'epsilon_decay={j}']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRQN vs DRQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: h=1, epsilon_decay=0.95\n",
      "playing times: 3000\n",
      "length of loss: 2936, average of loss (interval is 2): 0.4487104880408006, average of loss (interval is 20): 0.47947113510663936, average of loss (interval is 100): 0.3961803452776318\n",
      "length of loss: 2936, average of loss (interval is 2): 0.6912727479154854, average of loss (interval is 20): 0.7411777464347163, average of loss (interval is 100): 0.5318989552227625\n",
      "==================================================\n",
      "Your action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "Oppo action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "\n",
      "\n",
      "config: h=1, epsilon_decay=0.99\n",
      "playing times: 3000\n",
      "length of loss: 2936, average of loss (interval is 2): 0.27358664700737984, average of loss (interval is 20): 0.2932020850775789, average of loss (interval is 100): 0.28660630601807496\n",
      "length of loss: 2936, average of loss (interval is 2): 0.19517364247438146, average of loss (interval is 20): 0.20647403816038684, average of loss (interval is 100): 0.18264312187893667\n",
      "==================================================\n",
      "Your action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "Oppo action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "\n",
      "\n",
      "config: h=1, epsilon_decay=0.995\n",
      "playing times: 3000\n",
      "length of loss: 2936, average of loss (interval is 2): 0.26499761756991924, average of loss (interval is 20): 0.27955686015561354, average of loss (interval is 100): 0.25503332601359335\n",
      "length of loss: 2936, average of loss (interval is 2): 0.2756237923013955, average of loss (interval is 20): 0.29330696658421185, average of loss (interval is 100): 0.31546437801177185\n",
      "==================================================\n",
      "Your action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "Oppo action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "\n",
      "\n",
      "config: h=1, epsilon_decay=0.999\n",
      "playing times: 6000\n",
      "length of loss: 5936, average of loss (interval is 2): 0.35412333163722387, average of loss (interval is 20): 0.36276630994708786, average of loss (interval is 100): 0.3557057396813131\n",
      "length of loss: 5936, average of loss (interval is 2): 0.3572360842237921, average of loss (interval is 20): 0.37377375926117146, average of loss (interval is 100): 0.3419365524373764\n",
      "==================================================\n",
      "Your action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "Oppo action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "\n",
      "\n",
      "config: h=2, epsilon_decay=0.95\n",
      "playing times: 3000\n",
      "length of loss: 2935, average of loss (interval is 2): 0.10476446242812772, average of loss (interval is 20): 0.09703158120927441, average of loss (interval is 100): 0.11015376218468494\n",
      "length of loss: 2935, average of loss (interval is 2): 0.10489440566535037, average of loss (interval is 20): 0.09747629947953386, average of loss (interval is 100): 0.11950091756884831\n",
      "==================================================\n",
      "Your action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "Oppo action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "\n",
      "\n",
      "config: h=2, epsilon_decay=0.99\n"
     ]
    }
   ],
   "source": [
    "result_dict_drqn={'h=1':{}, 'h=2':{}, 'h=5':{}, 'h=10':{}}\n",
    "loss_dict_drqn={'h=1':{}, 'h=2':{}, 'h=5':{}, 'h=10':{}}\n",
    "strategy_dict_drqn={'h=1':{}, 'h=2':{}, 'h=5':{}, 'h=10':{}}\n",
    "\n",
    "for key in result_dict_drqn:\n",
    "    result_dict_drqn[key] = copy.deepcopy(epsilon_dict)\n",
    "    loss_dict_drqn[key] = copy.deepcopy(epsilon_dict)\n",
    "    strategy_dict_drqn[key] = copy.deepcopy(epsilon_dict)\n",
    "\n",
    "for i in h:\n",
    "    for j in epsilon_decay:\n",
    "        config['h'] = i\n",
    "        config['epsilon_decay'] = j\n",
    "        config_ob = Config(config)\n",
    "        env = Environment(config_ob)\n",
    "        print(f'config: h={i}, epsilon_decay={j}')\n",
    "        \n",
    "        rl_num = 10\n",
    "        for _ in range(5):\n",
    "            convergence = False\n",
    "            with HiddenPrints():\n",
    "                agent1 = constructAgent(strategies[rl_num], config_ob)\n",
    "                agent2 = constructAgent(strategies[rl_num], config_ob)\n",
    "\n",
    "                k = 1000\n",
    "                while not convergence:\n",
    "                    env.play(agent1, agent2, k)\n",
    "                    convergence = determine_convergence(agent2, 20, k=k)\n",
    "                    if agent2.play_times >= 20*k:\n",
    "                        break\n",
    "        \n",
    "                result_dict_drqn[f'h={i}'][f'epsilon_decay={j}'].append(agent2.play_times)\n",
    "                loss_dict_drqn[f'h={i}'][f'epsilon_decay={j}'].append([np.mean(agent1.loss),np.mean(agent2.loss)])\n",
    "                strategy_dict_drqn[f'h={i}'][f'epsilon_decay={j}'].append([agent1.own_memory[agent1.play_times-10:agent1.play_times].numpy().astype(int).tolist(), agent2.own_memory[agent2.play_times-10:agent2.play_times].numpy().astype(int).tolist()])\n",
    "        print(f'playing times: {agent2.play_times}')\n",
    "        print(f'length of loss: {len(agent1.loss)}, average of loss (interval is 2): {np.mean(agent1.loss[::2])}, average of loss (interval is 20): {np.mean(agent1.loss[::20])}, average of loss (interval is 100): {np.mean(agent1.loss[::100])}')\n",
    "        print(f'length of loss: {len(agent2.loss)}, average of loss (interval is 2): {np.mean(agent2.loss[::2])}, average of loss (interval is 20): {np.mean(agent2.loss[::20])}, average of loss (interval is 100): {np.mean(agent2.loss[::100])}')\n",
    "        agent2.show()\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict_drqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strategy_dict_drqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in h:\n",
    "    for j in epsilon_decay:\n",
    "        print(np.mean(np.array(loss_dict_drqn[f'h={i}'][f'epsilon_decay={j}'])[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLearning vs TfT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict_q={'h=1':{}, 'h=2':{}, 'h=5':{}, 'h=10':{}}\n",
    "strategy_dict_q={'h=1':{}, 'h=2':{}, 'h=5':{}, 'h=10':{}}\n",
    "\n",
    "for key in result_dict_q:\n",
    "    result_dict_q[key] = copy.deepcopy(epsilon_dict)\n",
    "    strategy_dict_q[key] = copy.deepcopy(epsilon_dict)\n",
    "\n",
    "for i in h:\n",
    "    for j in epsilon_decay:\n",
    "        config['h'] = i\n",
    "        config['epsilon_decay'] = j\n",
    "        config_ob = Config(config)\n",
    "        env = Environment(config_ob)\n",
    "        print(f'config: h={i}, epsilon_decay={j}')\n",
    "        \n",
    "        n_num = 2\n",
    "        rl_num = 7\n",
    "        for _ in range(5):\n",
    "            convergence = False\n",
    "            with HiddenPrints():\n",
    "                agent1 = constructAgent(strategies[n_num], config_ob)\n",
    "                agent2 = constructAgent(strategies[rl_num], config_ob)\n",
    "\n",
    "                k = 1000\n",
    "                while not convergence:\n",
    "                    env.play(agent1, agent2, k)\n",
    "                    convergence = determine_convergence(agent2, 20, k=k)\n",
    "                    if agent2.play_times >= 20*k:\n",
    "                        break\n",
    "        \n",
    "                result_dict_q[f'h={i}'][f'epsilon_decay={j}'].append(agent2.play_times)\n",
    "                strategy_dict_q[f'h={i}'][f'epsilon_decay={j}'].append(list(agent2.own_memory[agent2.play_times-10:agent2.play_times]))\n",
    "        print(f'playing times: {agent2.play_times}')\n",
    "        agent2.show()\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRQN-Varient vs TfT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choices = {'0-alwaysCooperate','1-alwaysDefect','2-titForTat','3-reverseTitForTat','4-random','5-grudger','6-pavlov','7-qLearning','8-lstm-TFT','9-dqn','10-lstmqn','11-a2c','12-a2c-lstm'}\n",
    "# rl_choices = {'7-qLearning','8-lstm-pavlov','9-dqn','10-lstmqn','11-a2c','12-a2c-lstm'}\n",
    "strategies = {0:'ALLC',1:'ALLD',2:'TitForTat',3:'revTitForTat',4:'Random',5:'Grudger',6:'Pavlov',7:'QLearning',8:'LSTM',9:'DQN',10:'LSTMQN',11:'A2C',12:'A2CLSTM'}\n",
    "\n",
    "h = [1,2,5,10]\n",
    "epsilon_decay = [0.95, 0.99, 0.995, 0.999]\n",
    "\n",
    "config = {\n",
    "    'reward': 3, \n",
    "    'sucker': 0, \n",
    "    'temptation': 5, \n",
    "    'punishment': 1, \n",
    "    'n_episodes': 10000, \n",
    "    'discount': 0.99,\n",
    "    'play_epsilon': 1,\n",
    "    'select_epsilon': 1,\n",
    "    'epsilon_decay': 0.999,\n",
    "    'min_epsilon': 0.01,\n",
    "    'alpha': 0.1,\n",
    "    'n_actions': 2,\n",
    "    'h': 10,\n",
    "    'state_repr': 'bi-repr',\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 1e-3,\n",
    "}\n",
    "\n",
    "result_dict_drqnv_t={'h=1':{}, 'h=2':{}, 'h=5':{}, 'h=10':{}}\n",
    "loss_dict_drqnv_t={'h=1':{}, 'h=2':{}, 'h=5':{}, 'h=10':{}}\n",
    "strategy_dict_drqnv_t={'h=1':{}, 'h=2':{}, 'h=5':{}, 'h=10':{}}\n",
    "                \n",
    "for key in result_dict_drqnv_t:\n",
    "    result_dict_drqnv_t[key] = copy.deepcopy(epsilon_dict)\n",
    "    loss_dict_drqnv_t[key] = copy.deepcopy(epsilon_dict)\n",
    "    strategy_dict_drqnv_t[key] = copy.deepcopy(epsilon_dict)\n",
    "\n",
    "for i in h:\n",
    "    for j in epsilon_decay:\n",
    "        config['h'] = i\n",
    "        config['epsilon_decay'] = j\n",
    "        config_ob = Config(config)\n",
    "        env = Environment(config_ob)\n",
    "        print(f'config: h={i}, epsilon_decay={j}')\n",
    "        \n",
    "        num = 2\n",
    "        rl_num = 10\n",
    "        for _ in range(5):\n",
    "            convergence = False\n",
    "            # twoSimulate(dict({num: strategies[num],rl_num: strategies[rl_num]}), rl_num, config)\n",
    "            with HiddenPrints():\n",
    "                agent1 = constructAgent(strategies[num], config_ob)\n",
    "                agent2 = constructAgent(strategies[rl_num], config_ob)\n",
    "\n",
    "                k = 1000\n",
    "                while not convergence:\n",
    "                    env.play(agent1, agent2, k)\n",
    "                    convergence = determine_convergence(agent2, 20, k=k)\n",
    "                    if agent2.play_times >= 20*k:\n",
    "                        break\n",
    "        \n",
    "                result_dict_drqnv_t[f'h={i}'][f'epsilon_decay={j}'].append(agent2.play_times)\n",
    "                loss_dict_drqnv_t[f'h={i}'][f'epsilon_decay={j}'].append(np.mean(agent2.loss))\n",
    "                strategy_dict_drqnv_t[f'h={i}'][f'epsilon_decay={j}'].append(list(agent2.own_memory[agent2.play_times-10:agent2.play_times]))\n",
    "        print(f'playing times: {agent2.play_times}')\n",
    "        print(f'length of loss: {len(agent2.loss)}, average of loss (interval is 2): {np.mean(agent2.loss[::2])}, average of loss (interval is 20): {np.mean(agent2.loss[::20])}, average of loss (interval is 100): {np.mean(agent2.loss[::100])}')\n",
    "        agent2.show()\n",
    "        print()\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict_drqnv_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in h:\n",
    "    for j in epsilon_decay:\n",
    "        strategy_dict_drqnv_t[f'h={i}'][f'epsilon_decay={j}'] = torch.Tensor(strategy_dict_drqnv_t[f'h={i}'][f'epsilon_decay={j}']).numpy().astype(int).tolist()\n",
    "    print(f'h={i}')\n",
    "    print(strategy_dict_drqnv_t[f'h={i}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in h:\n",
    "    for j in epsilon_decay:\n",
    "        print(np.mean(np.array(loss_dict_drqnv_t[f'h={i}'][f'epsilon_decay={j}'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRQN-Varient vs DRQN-Varient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict_drqnv={'h=1':{}, 'h=2':{}, 'h=5':{}, 'h=10':{}}\n",
    "loss_dict_drqnv={'h=1':{}, 'h=2':{}, 'h=5':{}, 'h=10':{}}\n",
    "strategy_dict_drqnv={'h=1':{}, 'h=2':{}, 'h=5':{}, 'h=10':{}}\n",
    "                \n",
    "for key in result_dict_drqnv:\n",
    "    result_dict_drqnv[key] = copy.deepcopy(epsilon_dict)\n",
    "    loss_dict_drqnv[key] = copy.deepcopy(epsilon_dict)\n",
    "    strategy_dict_drqnv[key] = copy.deepcopy(epsilon_dict)\n",
    "\n",
    "for i in h:\n",
    "    for j in epsilon_decay:\n",
    "        config['h'] = i\n",
    "        config['epsilon_decay'] = j\n",
    "        config_ob = Config(config)\n",
    "        env = Environment(config_ob)\n",
    "        print(f'config: h={i}, epsilon_decay={j}')\n",
    "        \n",
    "        rl_num = 10\n",
    "        for _ in range(5):\n",
    "            convergence = False\n",
    "            # twoSimulate(dict({num: strategies[num],rl_num: strategies[rl_num]}), rl_num, config)\n",
    "            with HiddenPrints():\n",
    "                agent1 = constructAgent(strategies[rl_num], config_ob)\n",
    "                agent2 = constructAgent(strategies[rl_num], config_ob)\n",
    "\n",
    "                k = 1000\n",
    "                while not convergence:\n",
    "                    env.play(agent1, agent2, k)\n",
    "                    convergence = determine_convergence(agent2, 20, k=k)\n",
    "                    if agent2.play_times >= 20*k:\n",
    "                        break\n",
    "        \n",
    "                result_dict_drqnv[f'h={i}'][f'epsilon_decay={j}'].append(agent2.play_times)\n",
    "                loss_dict_drqnv[f'h={i}'][f'epsilon_decay={j}'].append([np.mean(agent1.loss),np.mean(agent2.loss)])\n",
    "                strategy_dict_drqnv[f'h={i}'][f'epsilon_decay={j}'].append([agent1.own_memory[agent1.play_times-10:agent1.play_times].numpy().astype(int).tolist(), agent2.own_memory[agent2.play_times-10:agent2.play_times].numpy().astype(int).tolist()])\n",
    "        print(f'playing times: {agent2.play_times}')\n",
    "        print(f'length of loss: {len(agent1.loss)}, average of loss (interval is 2): {np.mean(agent1.loss[::2])}, average of loss (interval is 20): {np.mean(agent1.loss[::20])}, average of loss (interval is 100): {np.mean(agent1.loss[::100])}')\n",
    "        print(f'length of loss: {len(agent2.loss)}, average of loss (interval is 2): {np.mean(agent2.loss[::2])}, average of loss (interval is 20): {np.mean(agent2.loss[::20])}, average of loss (interval is 100): {np.mean(agent2.loss[::100])}')\n",
    "        agent2.show()\n",
    "        print()\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict_drqnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_dict_drqnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in h:\n",
    "    for j in epsilon_decay:\n",
    "        print(np.mean(np.array(loss_dict_drqnv[f'h={i}'][f'epsilon_decay={j}'])[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
